{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24687b97c8144cdc85b0e5a16f79a3de",
     "grade": false,
     "grade_id": "cell-b8656eb275ad5534",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1\n",
    "### Natural Language Processing and Information Extraction,  2020 WS\n",
    "\n",
    "\n",
    "_This assignment was created by [Judit Acs](http://juditacs.github.io/) for [this course](https://github.com/bmeaut/python_nlp_2018_spring) and is reused with her permission._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6056603fbaf4cefdd4c89820ab426bde",
     "grade": false,
     "grade_id": "cell-c45bf61615aa3247",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "This assignment involves implementing a simple part-of-speech tagger in Python, using the Viterbi algorithm.\n",
    "The exercises should be solved in the order presented in this notebook.\n",
    "\n",
    "Most exercises include tests which will pass if your solution is correct, but successful test do not guarantee that your solution is correct and some additional tests will be performed on your code after submission.\n",
    "\n",
    "Test cells cannot be modified and empty cells cannot be deleted. Your solution should replace placeholder lines such as:\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "You don't need to write separate functions except when the function header is predefined.\n",
    "Variable names must be derived from the public test.\n",
    "Please do not add new cells, they will be ignored by the autograder.\n",
    "\n",
    "### IMPORTANT\n",
    "Before submitting your solution,\n",
    "run your notebook with `Kernel -> Restart & Run All` and make sure it\n",
    "runs without exceptions.\n",
    "**If your code fails the public tests (the ones you see), you will automatically receive 0 points for that exercise.**\n",
    "\n",
    "## Submission\n",
    "\n",
    "Please see the separate set of instructions on TUWEL on how to submit your solution. Note that you can submit multiple times before the deadline, but only your last submission will be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dc238e6a6d85e49b6ea30a7cc0f58c4",
     "grade": false,
     "grade_id": "cell-88356aac3c2fbd17",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "716726e0cb25f8a545065e9b7de0e4ef",
     "grade": false,
     "grade_id": "cell-f7e07a713f45fc26",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Data setup\n",
    "\n",
    "You will work with a sample of appr. 1 million words from the UMBC WebBase corpus.\n",
    "There is also a smaller toy sample of a 100 sentences used for the public test and quick debugging.\n",
    "Both files are prepackaged as a zip file in your starter repository.\n",
    "\n",
    "If you are running the code from somewhere else and the data is not there, it will be downloaded and unpacked.\n",
    "You can set the data directory by changing the value of the `HW2_DATA_DIR` environmental variable.\n",
    "\n",
    "If you just cloned your starter repository in the recommended way, you should see the data extracted on the first run and no output on later runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d853a6de28a5fb54d07dbc1cdae0a562",
     "grade": false,
     "grade_id": "cell-88356aac3c2fbd18",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# setting up data directory (used by the instructor)\n",
    "\n",
    "data_dir = os.getenv(\"HW2_DATA_DIR\")\n",
    "if data_dir is None:\n",
    "    data_dir = \"\"\n",
    "    \n",
    "data_zip_path = os.path.join(data_dir, \"data.zip\")\n",
    "\n",
    "if not os.path.exists(data_zip_path):\n",
    "    print(\"Download data\")\n",
    "    import urllib.request\n",
    "    u = urllib.request.URLopener()\n",
    "    u.retrieve(\"http://avalon.aut.bme.hu/~judit/resources/umbc/data.zip\", data_zip_path)\n",
    "    print(\"Data downloaded\")\n",
    "\n",
    "unzip_path = os.path.join(data_dir, \"umbc_sample_toy.txt\")\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, \"umbc_sample_toy.txt\")) or \\\n",
    "        not os.path.exists(os.path.join(data_dir, \"umbc_sample_1M.txt\")):\n",
    "    print(\"Extracting data\")\n",
    "    from zipfile import ZipFile\n",
    "    with ZipFile(data_zip_path) as myzip:\n",
    "        myzip.extractall(data_dir)\n",
    "    print(\"Data extraction done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "301e56fa0e3fdb27dd1a913f04300aa8",
     "grade": false,
     "grade_id": "cell-70ef199bee1add72",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 1. Write a generator function that reads a text corpus. (10 points)\n",
    "\n",
    "Each sentence occupies two lines. The first line is the sentence itself. Tokens (words) are separated by spaces.\n",
    "The second line is the POS (part-of-speech) tag of each token, also separated by space.\n",
    "Then comes an empty line before the next sentence.\n",
    "The end of the file may or may not contain an empty line.\n",
    "\n",
    "```\n",
    "This is a sentence .\n",
    "DT VBZ DT NN .\n",
    "\n",
    "This is the next sentence .\n",
    "DT VBZ DT JJ NN .\n",
    "```\n",
    "\n",
    "Your task is to create a generator function with one parameter, the file's name.\n",
    "The generator should yield one sentence at a time.\n",
    "\n",
    "A sentence is a list of token-POS tag pairs (tuples), so the first sentence in the example would look like this:\n",
    "\n",
    "```\n",
    "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n",
    "```\n",
    "\n",
    "and the second sentence:\n",
    "\n",
    "```\n",
    "[('This', 'DT'), ('is', 'VBZ'), ('the', 'DT'), ('next', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n",
    "\n",
    "```\n",
    "Some sentences may be malformatted, and the number of tokens and POS tags differ, **skip** these sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98001bcc0eaebab95b6948943858b51a",
     "grade": false,
     "grade_id": "cell-4d6620881903f657",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_corpus(fn):\n",
    "    with open(fn) as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            tokens = re.split(r'(?<!\\d) | (?!\\d\\/\\d)', line)\n",
    "            pos_tags = next(file).strip().split()\n",
    "            \n",
    "            if len(tokens) == len(pos_tags):\n",
    "                yield list(zip(tokens, pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90e3352f4deada1f96c4ceea8916d05e",
     "grade": true,
     "grade_id": "cell-be0636a354bd3095",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "toy_data_fn = os.path.join(data_dir, \"umbc_sample_toy.txt\")\n",
    "\n",
    "toy_sentences = read_corpus(toy_data_fn)\n",
    "\n",
    "assert isinstance(toy_sentences, types.GeneratorType)\n",
    "\n",
    "toy_sentences = list(toy_sentences)\n",
    "\n",
    "# there are invalid sentences in the data, that you should skip\n",
    "assert len(toy_sentences) < 100\n",
    "assert toy_sentences[0] == [('Then', 'RB'),\n",
    " ('Jon', 'NNP'),\n",
    " ('falls', 'VBZ'),\n",
    " ('for', 'IN'),\n",
    " ('the', 'DT'),\n",
    " ('pretty', 'JJ'),\n",
    " ('new', 'JJ'),\n",
    " ('vet', 'NN'),\n",
    " ('played', 'VBN'),\n",
    " ('by', 'IN'),\n",
    " ('Jennifer', 'NNP'),\n",
    " ('Love', 'NNP'),\n",
    " ('Hewitt', 'NNP'),\n",
    " ('.', '.')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffe0b6f2feaade661ade0acda219a430",
     "grade": false,
     "grade_id": "cell-74a7f4cfbd6c9f92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 2. Basic statistics (15 points)\n",
    "\n",
    "Compute basic statistics on the corpus. Try to iterate over the file only once and compute all of these statistics as you go.\n",
    "\n",
    "The statistics you are supposed to compute and store in the appropriate variables (see the tests below):\n",
    "\n",
    "1. number of sentences,\n",
    "2. number of tokens (words),\n",
    "3. number of types (unique words),\n",
    "4. length of the longest sentence,\n",
    "5. length of the shortest sentence,\n",
    "6. word frequencies,\n",
    "7. POS frequencies.\n",
    "\n",
    "You are **NOT** allowed to use `collections.Counter` but you are free to use `collections.defaultdict`.\n",
    "\n",
    "Use the variable `umbc_fn` as a parameter of `read_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a52e088d67b00b080949ec2bc464364",
     "grade": false,
     "grade_id": "cell-8ea5ae29dee1adee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "umbc_fn = os.path.join(data_dir, \"umbc_sample_1M.txt\")\n",
    "\n",
    "sentence_no = 0\n",
    "token_no = 0\n",
    "\n",
    "sentence_maxlen = 0\n",
    "sentence_minlen = np.inf\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "pos_freq = defaultdict(int)\n",
    "\n",
    "for sentence in read_corpus(umbc_fn):\n",
    "    sentence_no += 1\n",
    "    \n",
    "    sentence_maxlen = max(sentence_maxlen, len(sentence))\n",
    "    sentence_minlen = min(sentence_minlen, len(sentence))\n",
    "    \n",
    "    for token, pos_tag in sentence:\n",
    "        token_no += 1\n",
    "        \n",
    "        unique_tokens.add(token)\n",
    "        \n",
    "        word_freq[token] += 1\n",
    "        pos_freq[pos_tag] += 1\n",
    "    \n",
    "type_no = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a47ec648f97b74a5ef36c418242f6ad2",
     "grade": false,
     "grade_id": "cell-335896cfa3e4591b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 41989\n",
      "Number of tokens (words): 969128\n",
      "Number of types (unique words)): 53727\n",
      "Length of the longest sentence: 131\n",
      "Length of the shortest sentence: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of sentences: {}\".format(sentence_no))\n",
    "print(\"Number of tokens (words): {}\".format(token_no))\n",
    "print(\"Number of types (unique words)): {}\".format(type_no))\n",
    "print(\"Length of the longest sentence: {}\".format(sentence_maxlen))\n",
    "print(\"Length of the shortest sentence: {}\".format(sentence_minlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0929a5aa0f305284e02695c7e9a8741",
     "grade": true,
     "grade_id": "cell-ea06292eacbe2c42",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(sentence_no) == int\n",
    "assert type(token_no) == int\n",
    "assert type(type_no) == int\n",
    "assert type(sentence_maxlen) == int\n",
    "assert type(sentence_minlen) == int\n",
    "assert sentence_no == 41989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "293a9867c4d3f2303f33e27ef0856a5e",
     "grade": true,
     "grade_id": "cell-b6f744a5709a2432",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(word_freq) == defaultdict or type(word_freq) == dict\n",
    "assert type(pos_freq) == defaultdict or type(pos_freq) == dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8daa3cfd79544ab856e915f394410751",
     "grade": false,
     "grade_id": "cell-c5a14ee2664be3c0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.2 What are the 10 most frequent words and 5 most frequent POS tags.\n",
    "\n",
    "The lists should be in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83ece5544a32831c44a2c790856d833e",
     "grade": false,
     "grade_id": "cell-8f793d4194c41524",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "most_frequent_words = sorted(word_freq, key=word_freq.get, reverse=True)[:10]\n",
    "most_frequent_pos = sorted(pos_freq, key=pos_freq.get, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "369d98141886e36f11bb514830533944",
     "grade": true,
     "grade_id": "cell-dc0e2c719dc647cf",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', '.', 'of', 'a', 'and', 'to', 'in', \"'s\", 'is']\n",
      "['NN', 'IN', 'DT', 'JJ', 'NNP']\n"
     ]
    }
   ],
   "source": [
    "print(most_frequent_words)\n",
    "print(most_frequent_pos)\n",
    "assert type(most_frequent_words) == list\n",
    "assert type(most_frequent_pos) == list\n",
    "assert len(most_frequent_words) == 10\n",
    "assert type(most_frequent_words[0]) == str\n",
    "assert most_frequent_pos[0] == 'NN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "db5b99a91a43698fc06cf4ab029ba86a",
     "grade": false,
     "grade_id": "cell-2fcc8f5b2e06a1bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 3. Hapax legomenon (10 points)\n",
    "\n",
    "Hapax legomenon or hapax is a word that only appears once in the dataset. Since the distribution of words roughly follows Zipf's distribution, regardless of the size of the corpus, there will always be a large number of hapax or words that only appear once.\n",
    "\n",
    "Aside from theoretical, there are technical reasons for filtering these words such as the fact that we have limited memory.\n",
    "\n",
    "Compute the number of hapax words in the corpus using your previous statistics.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0183ff46ba8ecb935cc4c38a46bff12",
     "grade": false,
     "grade_id": "cell-2ab659ccf17f6757",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "hapax_no = sum(freq == 1 for freq in word_freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa96fa83e7fbfcd9fccd030ad304f63f",
     "grade": true,
     "grade_id": "cell-6ac7967eb6f14ea8",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23711\n"
     ]
    }
   ],
   "source": [
    "print(hapax_no)\n",
    "assert type(hapax_no) == int\n",
    "assert 0 < hapax_no < len(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a1a8017140ce32dfd9600b2b0a03e6d",
     "grade": false,
     "grade_id": "cell-5b3dda2849c6204b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## The `_UNK_` symbol\n",
    "\n",
    "Rare words including hapax words are most often replaced by a common symbol such as `_UNK_` (unknown).\n",
    "During inference time (when running your model on unseen data), we may encounter words either never seen in the training data or deemed too rare and replaced with `_UNK_`.\n",
    "The best way to handle these words is to replace them with `_UNK_` since our model is prepared to handle these symbols.\n",
    "\n",
    "Follow this strategy in your viterbi function.\n",
    "\n",
    "Tip: you don't actually have to replace these words, you can check if they're in the `word_idx` dictionary. `dict.get` might be useful.\n",
    "\n",
    "## 3.1 Set of frequent words.\n",
    "\n",
    "Create a set of words that appear more at least `min_freq` times in the data.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74502be497732d073b9e416350023d97",
     "grade": false,
     "grade_id": "cell-2c844fe2043a74b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "\n",
    "top_words = set(word for word, freq in word_freq.items() if freq >= min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cca47235e37ed907afa5ad2621ad1f58",
     "grade": true,
     "grade_id": "cell-970d67a9a7d8aea2",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30016\n"
     ]
    }
   ],
   "source": [
    "print(len(top_words))\n",
    "assert type(top_words) == set\n",
    "assert 0 < len(top_words) < len(word_freq)\n",
    "assert \"dog\" in top_words\n",
    "assert \"titillating\" not in top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05c9f1e9ef16bef826da60b7bbb73a9f",
     "grade": false,
     "grade_id": "cell-b71d2d3a5451784e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 4. Mapping and counters (25 points)\n",
    "\n",
    "## 4.1 Word and POS mapping\n",
    "\n",
    "Create a word and a POS mapping that maps each word to its unique integer id. The word ids should range from 0 to `len(top_words)-1`, while the POS ids should range from 0 to `len(pos_idx)-1`.\n",
    "You only need to map the non-rare words.\n",
    "\n",
    "Include the unknown symbol (`_UNK_`) in the word mapping. What should its id be?\n",
    "\n",
    "Add a `START` symbol to the POS mapping. It will be used in the Viterbi algorithm.\n",
    "Do not read the file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c46ea36d4f198ebfefb3e851ef84abc",
     "grade": false,
     "grade_id": "cell-95dfecb8a30f7124",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "word_idx = {word: i for i, word in enumerate(top_words)}\n",
    "word_idx['_UNK_'] = len(word_idx)\n",
    "\n",
    "pos_idx = {pos_tag: i for i, pos_tag in enumerate(pos_freq)}\n",
    "pos_idx['START'] = len(pos_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ee5307b939c4ce606a00ab7fa3e35468",
     "grade": true,
     "grade_id": "cell-5d34e89ea9a099df",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(word_idx) == len(top_words) + 1  # _UNK_ included\n",
    "assert '_UNK_' in word_idx\n",
    "assert 'START' in pos_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04576b1eb782192b806a087b0df0ac9f",
     "grade": false,
     "grade_id": "cell-a89c3e0ad5a13921",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4.2 Emission and transition counts\n",
    "\n",
    "\n",
    "Hidden Markov Models were introduced in the lecture:\n",
    "\n",
    "\n",
    "* Like the Markov model, we take only the $n$ preceding tokens into consideration\n",
    "* The idea behind the model is very different:\n",
    "    * We imagine an automaton that is always in a **(hidden) state**\n",
    "    * In each state, it emits something we can observe\n",
    "    * The task is to find out which is _the most probable_ state sequence that generates the observations\n",
    "* In the POS tagging context,\n",
    "    * The words in the text are the **observed events**\n",
    "    * The POS tags are the hidden states\n",
    "    \n",
    "    \n",
    "You will build a simple Hidden Markov Model for English POS tagging.\n",
    "In this case $n$ equals 1, so only the previous token is taken into consideration.\n",
    "\n",
    "Transition and emission probabilities are derived from the corpus. Compute the emission and transition counts from the corpus. Do not iterate over the data more than once.\n",
    "\n",
    "You need to build two matrices as follows ($\\#$ means _count of_):\n",
    "\n",
    "* a $|L| \\times |V|$ matrix of integers\n",
    "  $$M(i,j) = \\# \\ i^\\text{th} \\text{ pos tag emitting word } j$$\n",
    "* an $|L|\\times |L|$ matrix of integers\n",
    "  $$N(i,j) = \\# \\ j^\\text{th} \\text{ pos after } i^\\text{th} \\text { pos}$$\n",
    "  \n",
    "**CAUTION** Both matrices are defined differently from the ones in the lab exercise.\n",
    "\n",
    "### How to handle the POS tag of the first word in a sentence?\n",
    "\n",
    "HMMs need an explicit way of handling starting probabilities which is usually solved with special start state(s).\n",
    "Build the transition matrix as if there is an artificial `START` state at the beginning of every sentence.\n",
    "You don't actually have to add a symbol to the sentence.\n",
    "The start state does not affect the emission probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa0243ae2ca228f27048a8c0f70f20f2",
     "grade": false,
     "grade_id": "cell-db601053694db0b8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "emission_counts = np.zeros((len(pos_idx), len(word_idx)), dtype=int)\n",
    "transition_counts = np.zeros((len(pos_idx), len(pos_idx)), dtype=int)\n",
    "\n",
    "for sentence in read_corpus(umbc_fn):\n",
    "    i = pos_idx['START']\n",
    "    \n",
    "    for token, pos_tag in sentence:\n",
    "        j = pos_idx[pos_tag]\n",
    "        transition_counts[i, j] += 1\n",
    "        \n",
    "        i, j = j, word_idx.get(token, word_idx['_UNK_'])\n",
    "        emission_counts[i, j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3870da560d00c879783409317d546ac",
     "grade": true,
     "grade_id": "cell-abf2adc1dbec1bd4",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 30017)\n",
      "(46, 46)\n",
      "The state NN emitted the word dog 64 times\n",
      "The state NN followed the state VBZ 8113 times\n"
     ]
    }
   ],
   "source": [
    "print(emission_counts.shape)\n",
    "print(transition_counts.shape)\n",
    "assert(emission_counts[pos_idx[\"NN\"], word_idx[\"_UNK_\"]] == 4717)\n",
    "\n",
    "dog_id = word_idx[\"dog\"]\n",
    "noun_id = pos_idx[\"NN\"]\n",
    "vbz_id = pos_idx[\"VBZ\"]\n",
    "noun_dog_count = emission_counts[noun_id, dog_id]\n",
    "assert noun_dog_count == 64\n",
    "print(\"The state NN emitted the word dog {} times\".format(emission_counts[noun_id, dog_id]))\n",
    "print(\"The state NN followed the state VBZ {} times\".format(transition_counts[noun_id, vbz_id]))\n",
    "\n",
    "assert type(emission_counts) == np.ndarray\n",
    "assert type(transition_counts) == np.ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89bb625f6a0232f2f9258ec742e89bf8",
     "grade": false,
     "grade_id": "cell-8558b5090e3cb8bd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4.3 Emission and transition probabilities\n",
    "\n",
    "Empirical emission and transition probabilities are defined as\n",
    "\n",
    "* a $|L|\\times |V|$ matrix of floats\n",
    "  $$P_1(i,j) = \\frac{\\# \\ i^\\text{th} \\text{ pos tag emitting word } j}{\\# \\ \\text{pos tag } i}$$\n",
    "* an $|L|\\times |L|$ matrix of floats\n",
    "  $$P_2(i,j) = \\frac{\\# \\ j^\\text{th} \\text{ pos after } i^\\text{th} \\text { pos}}{\\# \\ \\text{pos tag } i}$$\n",
    "\n",
    "Create these probability matrices using the transition and emission counts from the previous exercise.\n",
    "\n",
    "You should see a warning for emission probabilities since one of the rows sums to 0.\n",
    "It is the row corresponding to the `START` state.\n",
    "Add an artificial emission of the `_UNK_` symbol from the `START` state to solve this (see the tests below).\n",
    "\n",
    "Use 64-bit floats.\n",
    "Do not use log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "489830168102e0c707f8581f0ef84e8b",
     "grade": false,
     "grade_id": "cell-4b6fe7c5c4a718d2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "emission_probs = np.zeros((len(pos_idx), len(word_idx)), dtype=np.float64)\n",
    "transition_probs = np.zeros((len(pos_idx), len(pos_idx)), dtype=np.float64)\n",
    "\n",
    "emission_counts[pos_idx['START'], word_idx['_UNK_']] = 1\n",
    "\n",
    "for pos_tag, i in pos_idx.items():\n",
    "    total_count = sum(emission_counts[i])\n",
    "    for j in word_idx.values():\n",
    "        emission_probs[i, j] = emission_counts[i, j] / total_count\n",
    "    \n",
    "    total_count = sum(transition_counts[i])\n",
    "    for j in pos_idx.values():\n",
    "        transition_probs[i, j] = transition_counts[i, j] / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e77fc01f1b0bdc025405a523ff7d6f80",
     "grade": true,
     "grade_id": "cell-3fd3068b4a7fad9e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.count_nonzero(emission_counts[pos_idx['START']]) != 0\n",
    "assert type(emission_probs) == np.ndarray\n",
    "assert type(transition_probs) == np.ndarray\n",
    "assert emission_counts.shape == emission_probs.shape\n",
    "assert transition_counts.shape == transition_probs.shape\n",
    "assert np.allclose(emission_probs.sum(axis=1), 1)\n",
    "assert np.allclose(transition_probs.sum(axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a617bbef6d94a03a507ba3a9646b5377",
     "grade": false,
     "grade_id": "cell-92a83f6bfe1636d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# 5. Viterbi algorithm (30 points)\n",
    "\n",
    "Implement the Viterbi algorithm for $n=1$ (the model only takes into account the previous state).\n",
    "\n",
    "The `viterbi` function takes five parameters:\n",
    "1. the sentence as a list of tokens,\n",
    "2. the transition probability matrix,\n",
    "3. the emission probability matrix,\n",
    "4. the word - id mapping, \n",
    "5. and the POS tag - id mapping,\n",
    "\n",
    "and it returns a list of POS tags.\n",
    "It is the same length as the input sentence and each POS tag corresponds to one token in the input sentence.\n",
    "\n",
    "Your function should handle out-of-vocabulary words as if they're `_UNK_` symbols.\n",
    "\n",
    "Remember that the HMM always starts from an artificial `START` stat, then proceeds to the first _real_ state (POS in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65de9f601fef0be9373219d179450b70",
     "grade": false,
     "grade_id": "cell-834f42f6a029e60b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, transitions, emissions, word_idx, pos_idx):\n",
    "    viterbi_probs = np.zeros((len(pos_idx), len(sentence)), dtype=np.float64)\n",
    "    backpointers = np.zeros((len(pos_idx), len(sentence)), dtype=np.int)\n",
    "    \n",
    "    for j, token in enumerate(sentence):\n",
    "        word_id = word_idx.get(token, word_idx['_UNK_'])\n",
    "        \n",
    "        for pos_tag, i in pos_idx.items():\n",
    "            # initialize matrices if at start of sentence\n",
    "            if j == 0:\n",
    "                viterbi_probs[i, j] = transitions[pos_idx['START'], i] * emissions[i, word_id]\n",
    "                backpointers[i, j] = pos_idx['START']\n",
    "                continue\n",
    "            \n",
    "            probs = viterbi_probs[:, j - 1] * transitions[:, i] * emissions[i, word_id]\n",
    "            \n",
    "            viterbi_probs[i, j] = np.max(probs)\n",
    "            backpointers[i, j] = np.argmax(probs)\n",
    "    \n",
    "    # follow backpointers\n",
    "    curr_pointer = np.argmax(viterbi_probs[:, j])\n",
    "    \n",
    "    pos_idx_to_tag = {i: pos_tag for pos_tag, i in pos_idx.items()}\n",
    "    pos_tags = [pos_idx_to_tag[curr_pointer]]\n",
    "    \n",
    "    for j in range(len(sentence) - 1, 0, -1):\n",
    "        curr_pointer = backpointers[curr_pointer, j]\n",
    "        pos_tags.append(pos_idx_to_tag[curr_pointer])\n",
    "    \n",
    "    return pos_tags[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec54094e30045b350dc96973f6926638",
     "grade": true,
     "grade_id": "cell-729b37e27ef530af",
     "locked": true,
     "points": 30,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NN', 'VBZ', '.']\n",
      "Work should receive different POS tags\n",
      "My work here is done: NN\n",
      "I work here: VBP\n",
      "['DT', 'JJ', 'JJ', 'NN', 'VBZ', 'IN', 'DT', 'JJ', 'NN', '.']\n"
     ]
    }
   ],
   "source": [
    "def run_viterbi(sentence):\n",
    "    \"\"\"Call viterbi with global parameters from previous exercises\"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    return viterbi(sentence, transition_probs, emission_probs, word_idx, pos_idx)\n",
    "\n",
    "tags = run_viterbi(\"The cat runs .\")\n",
    "print(tags)\n",
    "assert tags == ['DT', 'NN', 'VBZ', '.']\n",
    "work1 = run_viterbi(\"My work here is done .\")\n",
    "work2= run_viterbi(\"I work here .\")\n",
    "print(\"Work should receive different POS tags\\n\"\n",
    "      \"My work here is done: {}\\n\"\n",
    "      \"I work here: {}\".format(work1[1], work2[1]))\n",
    "print(viterbi([\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"fox\", \".\"],\n",
    "              transition_probs, emission_probs, word_idx, pos_idx))\n",
    "\n",
    "assert run_viterbi(\"Everyone is dancing now .\") == ['NN', 'VBZ', 'VBG', 'RB', '.']\n",
    "assert run_viterbi(\"I like fish .\") == ['PRP', 'IN', 'NN', '.']\n",
    "assert run_viterbi(\"He who laughs last , did not get it .\") == ['PRP', 'WP', 'VBZ', 'JJ', ',', 'VBD', 'RB', 'VB', 'PRP', '.']\n",
    "assert run_viterbi(\"My favorite machine at the gym is the vending machine .\") == ['PRP$', 'JJ', 'NN', 'IN', 'DT', 'NN', 'VBZ', 'DT', 'VBG', 'NN', '.']\n",
    "assert run_viterbi(\"We have almost finished the assignment .\") == ['PRP', 'VBP', 'RB', 'VBN', 'DT', 'NN', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5e83cd31edc4db1bbef0ad42647e3b9",
     "grade": false,
     "grade_id": "cell-8cde8063b129a6d6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5.2 Addtional tests (extra 10 points)\n",
    "\n",
    "Add your own tests that demonstrate the effect of context in POS tagging such as the previous example, where _work_ has a different POS tag in different context.\n",
    "\n",
    "2 points / example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5320fd0e73a15865cc5ea4d21c126a78",
     "grade": true,
     "grade_id": "cell-7723e811eb219319",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop should receive different POS tags\n",
      "She enjoyed every last drop of coffee: NN\n",
      "I often drop my phone: VB\n",
      "\n",
      "Fly should receive different POS tags\n",
      "Go fly a kite: VBP\n",
      "The fly is on the wall: NN\n",
      "\n",
      "Can should receive different POS tags\n",
      "I can speak Finnish: MD\n",
      "He drank a can of Coke: NN\n",
      "\n",
      "Watch should receive different POS tags\n",
      "I often watch TV: VB\n",
      "Nico has a new watch: NN\n",
      "\n",
      "Right should receive different POS tags\n",
      "She is right: RB\n",
      "Take a right turn: JJ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "drop1 = run_viterbi(\"She enjoyed every last drop of coffee .\")\n",
    "drop2 = run_viterbi(\"I often drop my phone .\")\n",
    "print(\"Drop should receive different POS tags\\n\"\n",
    "      \"She enjoyed every last drop of coffee: {}\\n\"\n",
    "      \"I often drop my phone: {}\\n\".format(drop1[4], drop2[2]))\n",
    "\n",
    "# Example 2\n",
    "fly1 = run_viterbi(\"Go fly a kite .\")\n",
    "fly2 = run_viterbi(\"The fly is on the wall .\")\n",
    "print(\"Fly should receive different POS tags\\n\"\n",
    "      \"Go fly a kite: {}\\n\"\n",
    "      \"The fly is on the wall: {}\\n\".format(fly1[1], fly2[1]))\n",
    "\n",
    "# Example 3\n",
    "can1 = run_viterbi(\"I can speak Finnish .\")\n",
    "can2 = run_viterbi(\"He drank a can of Coke .\")\n",
    "print(\"Can should receive different POS tags\\n\"\n",
    "      \"I can speak Finnish: {}\\n\"\n",
    "      \"He drank a can of Coke: {}\\n\".format(can1[1], can2[3]))\n",
    "\n",
    "\n",
    "# Example 4\n",
    "watch1 = run_viterbi(\"I often watch TV .\")\n",
    "watch2 = run_viterbi(\"Nico has a new watch .\")\n",
    "print(\"Watch should receive different POS tags\\n\"\n",
    "      \"I often watch TV: {}\\n\"\n",
    "      \"Nico has a new watch: {}\\n\".format(watch1[2], watch2[4]))\n",
    "\n",
    "# Example 5\n",
    "right1 = run_viterbi(\"She is right .\")\n",
    "right2 = run_viterbi(\"Take a right turn .\")\n",
    "print(\"Right should receive different POS tags\\n\"\n",
    "      \"She is right: {}\\n\"\n",
    "      \"Take a right turn: {}\\n\".format(right1[2], right2[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f4d69f8d265645def9cb379e4706edf",
     "grade": false,
     "grade_id": "cell-9672fc9c7981f51d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# PEP8, Code cleanness (10 points)\n",
    "\n",
    "This cell is here for technical reasons, you will receive feedback on your code quality here. You do not need to write anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "edb95118c918b3d8f6660b7c6f363d80",
     "grade": true,
     "grade_id": "cell-075b12bb3e4c677f",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
