{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_9idNCFOfHrt",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02f0e4b3cf945137791f790f663ad352",
     "grade": false,
     "grade_id": "cell-8dfe3f55647f8387",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Homework 2 - Part 2\n",
    "### Natural Language Processing and Information Extraction,  2020 WS\n",
    "----\n",
    "Exercise largely based on input from [Adam Kovacs](adam.kovacs@tuwien.ac.at)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a80ba1489114137502b13f024189a95",
     "grade": false,
     "grade_id": "cell-87157a7a3373d661",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Instructions\n",
    "\n",
    "This exercise does not include tests, visible or not. However, should your code give errors that are not due to memoriy limitations on the JupyterHub server, we cannot spend time fixing them.\n",
    "\n",
    "Your solution should replace placeholder lines such as:\n",
    "\n",
    "`### YOUR CODE HERE\n",
    "raise NotImplementedError()`\n",
    "\n",
    "__IMPORTANT:__ Before submitting your solution, run your notebook with Kernel -> Restart & Run All and make sure it runs without exceptions. \n",
    "\n",
    "__Submission:__ You need to submit your exercise via the nbgrader interface. Note that you can submit multiple times before the deadline, but only your last submission will be graded.\n",
    "\n",
    "Do not change the name of the ipynb file. Submition without validation is possible.\n",
    "\n",
    "There is a total of __75 points__ to get for this exercise. Try to solve the exercise on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2afd7dac0c31783d2ad2d93311f4471e",
     "grade": false,
     "grade_id": "cell-6d3fff6078c421dd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise Description\n",
    "\n",
    "In this exercise you will make aquaintance with PyTorch's framework for textual data, [TorchText](https://pytorch.org/text/). PyTorch is a Python library optimized for working with tensors for Deep Learning. To refresh your knowledge about tensors, I recommend to read \"[The Poor Man's Introduction to Tensors](https://web2.ph.utexas.edu/~jcfeng/notes/Tensors_Poor_Man.pdf)\" by Justin C. Feng, a very concise introduction to tensors and operations with tensors.\n",
    "\n",
    "The necessary packages are already available on the JupyterHub.\n",
    "\n",
    "You are requred, in the following, to load the data into memory (Task 1), to design a simple Feed Forward Network (Task 2) and, then, modify it to an LSTM network (Task 3). The overall problem to solve is classification.\n",
    "\n",
    "You are given a set of news texts (AG_NEWS) from different sources, and you have to build a classifier to make predictions on the types of articles:\n",
    "*   World\n",
    "*   Sports\n",
    "* Business\n",
    "* Sci/Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edb98b5e311848be4d9e20a2d4098143",
     "grade": false,
     "grade_id": "cell-a92e5d3acd6d7b40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data loading, prerequisites\n",
    "\n",
    "The [TorchText](https://pytorch.org/text/) framework offers you a set of built-in methods for handling text data sets. It also offers you some data sets for experimentation.\n",
    "\n",
    "The `torchtext.data.Field` class is one of the main concepts of TorchText which defines how the text data is to be processed and how to transform it to a tensor (something close to an embedding).\n",
    "\n",
    "The `TEXT` variable will store the processing pipeline selected for the data set in this exercise. We pass `tokenize = 'spacy'` to our tokenizer telling it to to use SpaCy methods. The default tokenizer would otherwise be splitting the text on spaces.\n",
    "\n",
    "Similarly, `LABEL` is a wrapper that marks specific fields in the data to be labels.\n",
    "\n",
    "You can read [here](https://torchtext.readthedocs.io/en/latest/data.html) to understand the role `Fields` play in `torchtext`.\n",
    "\n",
    "If you need to install `torchtext`on your environment, please use version 4.0:\n",
    "`!pip install torchtext == 0.4`\n",
    "\n",
    "\n",
    "__Hint: it is a good idea to use Google Colab for this homework to be able to use GPUs. When you have completed the exercise, you can transfer the code to the JupyterHub.__ Take care that you do not rename notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LYSiNI7AgJYA",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e711bf879618bd0c5a1db0370d0ad32",
     "grade": false,
     "grade_id": "cell-8afa41a904e79b50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Import the necessary libraries for this exercise. The random seed initialization is necessary for reproducibility reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FWGHh0ExgGBa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "# other imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JRJ7Z8aJjP0l",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b761555dd7a30ca905cabc1b8eddfa82",
     "grade": false,
     "grade_id": "cell-4618cfa79bcabce9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Initialize a `TEXT` and a `LABEL` variable with `torchtext` objects that will represent our dataset. We also set the random seed value defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2cPsOVofjNFf"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy')\n",
    "LABEL = data.LabelField(dtype = torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "osCVTyt8jwhX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62dceb2c233d178f44e109cb80d9b094",
     "grade": false,
     "grade_id": "cell-e20f279cb4b9d53a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 1. Loading the dataset, preparation for a NN training/prediction phase\n",
    "### (total per task 1: 25 points)\n",
    "\n",
    "One of the advantages of using `torchtext` is that it has methods for paralellizing data processing on GPUs, by sending batches of input data that can be processed at the same time on a GPU (on a CPU, the processing happens sequentially per batch).\n",
    "\n",
    "Here you need to load the data set using the right iterators, to define the training, validation, and test portions of the data. \n",
    "\n",
    "The training data set (out of which you will set out a portion as a validation set) is in the file `dataset_train.csv`. The testing data set is in `dataset_test.csv` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_z9KCGf3j1Yn",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50419fe204c3b59fecc988a0179a8f55",
     "grade": false,
     "grade_id": "cell-af4d530a847990cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TASK 1.1  Load the data \n",
    "_(15 points)_\n",
    "\n",
    "Use data.TabularDataset to load in the data sets and split the train file into training and validation sets. Use the previously defined random seed when splitting the data set.\n",
    "\n",
    "Hint: use data.TabularDatset.splits method with the right parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "id": "Vi8ywPZkjx8W",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1df04b0b0f4237587f70be820a98dfc2",
     "grade": true,
     "grade_id": "cell-116b9972337ab762",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_dataset(TEXT, LABEL):\n",
    "    fields = [('label', LABEL), ('text', TEXT)]\n",
    "    \n",
    "    train, valid = data.TabularDataset('dataset_train.csv', 'csv', fields, skip_header=True).split(0.7)\n",
    "    test = data.TabularDataset('dataset_test.csv', 'csv', fields, skip_header=True)\n",
    "    \n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qluq9OVzjuD2"
   },
   "outputs": [],
   "source": [
    "train, valid, test = load_dataset(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DfL6frNpk6Ad",
    "outputId": "5516bdda-a05a-4bff-bdf0-c18e1fd644cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 84000\n",
      "Number of validation examples: 36000\n",
      "Number of testing examples: 7600\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train)}')\n",
    "print(f'Number of validation examples: {len(valid)}')\n",
    "print(f'Number of testing examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vy0KwhjHk8Jt"
   },
   "outputs": [],
   "source": [
    "assert len(train) == 84000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zYPAcmhxk_ci",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d43f28208f383192fa655553d3ff5489",
     "grade": false,
     "grade_id": "cell-a150c2358b4794b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Building a vocabulary for the textual data set we work on is an essential step. A vocabulary is a lookup table where every unique word has a corresponding index.\n",
    "\n",
    "This is done so our machine learning model can operate on numbers instead of strings. The indexes are then used to construct embeddings for our words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6B4-YHnak_x9"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train)  \n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbZcC0bxlCf1",
    "outputId": "02287ea2-b385-46d4-fd79-f9eb54baa5e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101328\n"
     ]
    }
   ],
   "source": [
    "print(len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wezwerfWlEEF",
    "outputId": "15f4ed41-6331-4a9a-cae0-52afb3fd3ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 177777), ('the', 124554), ('.', 93195), ('to', 82695), ('-', 78301), ('a', 68652), ('of', 68399), ('in', 64781), ('and', 47862), ('on', 38844), ('for', 33975), ('#', 32867), ('(', 28282), (' ', 26845), ('39;s', 21676), ('that', 19128), ('with', 17148), ('The', 16941), ('as', 16459), (')', 16270)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vlwtWnHAmbcX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92984179676eaf5be3edc365a6bf22a7",
     "grade": false,
     "grade_id": "cell-b5bf4f234343a40b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When words in the test set are not found in the vocabulary of the training set, there is a special type of token that will be used for them: `< unk >`\n",
    "\n",
    "For example, if the sentence was `\"This film is great and I savoured it\"` but if the word `savoured` is not in the vocabulary, it would become `\"This film is great and I < unk > it\"`.\n",
    "\n",
    "We feed batches into our model, one at a time. Within a batch all sentences need to be of the same size. To do that, the shorter sentences need to be padded. With `torchtext` this is done automatically, when batches are created.\n",
    "\n",
    "![unk-example](unk-example.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "a6YP23c1mlWx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6706d4f39ece92f70945453d29e6adad",
     "grade": false,
     "grade_id": "cell-cc31250c23bec597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TASK 1.2 Construct iterators on the training, validation and test data. \n",
    "_(10 points)_\n",
    "\n",
    "Batching data allows us to process data sets that are much larger than the GPU's RAM. Use the `data.BucketIterator` to shuffle and bucket the input sequences in sequences of similar length. \n",
    "\n",
    "Hint: Set the correct parameter values for the `splits` method: batch_size, sort_key (based on length), sort_within_batch and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "id": "cQ1G0II0lORU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe8ab2a37d7d57d0108765b5584933b1",
     "grade": true,
     "grade_id": "cell-4b840a8d5ec66691",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def construct_iterators(train, dev, test):\n",
    "  train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "      (train, dev, test), batch_size=BATCH_SIZE, sort_key=lambda x: len(x.text),\n",
    "      sort_within_batch=True, device=device\n",
    "  )\n",
    "  return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "c3yRXkeMmw57"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = construct_iterators(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UN0yLNi_nJ2c",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1dde402e7a66468a119e35e54968386",
     "grade": false,
     "grade_id": "cell-987c289a929a9375",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2. Build a Feed forward Neural Network for text classification\n",
    "### (total per task 2: 40 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xVWaqj_znZ0d",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e3533d12edd197e2af6a8bd1f561c5a",
     "grade": false,
     "grade_id": "cell-5c3b768a4bc973f7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this task, you are to define a Feed-forward Neural Network for text classification. The network should consist of: \n",
    "* An embedding layer: the input batch is passed through the embedding layer to get word embeddings.\n",
    "* Sentence/sequence representation: from the word embeddings produce one vector for the sentence. This could be done by taking the average of the word vectors.\n",
    "* Hidden layer: feed the sentence vector through a linear layer to produce the output vector.\n",
    "* Output layer: apply a softmax function on the output vector to produce probabilities for the labels.\n",
    "\n",
    "You will need to use a loss function, and an optimization parameter.\n",
    "\n",
    "Hints: Each batch, text, is a tensor of size `[sentence length, batch size]`. Look at the members of `torch.nn`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "994e2c9c5f2a43b74f8d455e2311cbcc",
     "grade": false,
     "grade_id": "cell-e0ef8f170b4a67f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(10 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "id": "S6zHwnIBmxMj",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c13cc537245875dacdaf8cadfe1903b8",
     "grade": true,
     "grade_id": "cell-06a9548e541afeb6",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import autograd\n",
    "\n",
    "# defined the architecture of the FNN\n",
    "class FNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, output_dim):\n",
    "        # defines the layers of the network.\n",
    "        super(FNNClassifier, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.hidden = nn.Linear(embedding_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # returns the probability distribution over the output layer (softmax)\n",
    "        sentence_vectors = torch.mean(self.embeddings(text), 0)\n",
    "        hidden = self.hidden(sentence_vectors)\n",
    "        log_probs = F.log_softmax(hidden, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mSZoQKR7m_1L"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "# how many dimensions should the word embeddings have\n",
    "EMBEDDING_DIM = 100\n",
    "# how many classes should the classifier put the texts into.\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "# the training model we will use is now the FNN.\n",
    "model = FNNClassifier(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b368f085fc61b446b99da34aeda3de63",
     "grade": false,
     "grade_id": "cell-17c44bf076f14191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before training, we have to select the learning rate and the loss functions for our FNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pTb9O_eJpVcn"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the learning rate optimizer, you can experiment with various optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Send the tensors to GPU if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_TYoQz9-pfJz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-EebczNMplsM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48d0d8d890fc74ac46d35aa43e8af847",
     "grade": false,
     "grade_id": "cell-4ca5cca179b7a989",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TASK 2.1 Implement the train and the evaluate functions.\n",
    "\n",
    "Now it's the time to `train` the network, then apply it to the test set by implementing an `evaluate` function.\n",
    "\n",
    "The `train` function must:\n",
    "- iterate throught the dataset with the given iterator, \n",
    "- get the output from the model\n",
    "- calculate the loss and the accuracy\n",
    "- Propagate backward the loss\n",
    "- And calculate epoch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36fb8ffd0929635b11efa8eb89c93bb4",
     "grade": false,
     "grade_id": "cell-c1a55c6d84052c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(7 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "id": "lOfwmxPJpggx",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4020f6c13bb1ed389fbece6e449f987",
     "grade": true,
     "grade_id": "cell-b28f96f7c7ed38a2",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(batch.text)\n",
    "        loss = criterion(outputs, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss\n",
    "        epoch_acc += class_accuracy(outputs, batch.label)\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tGkgq-Edpwgo",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e12bc38003a0985d5f574b63eb751b51",
     "grade": false,
     "grade_id": "cell-ebc0ee573bbfd87d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The `evaluate`function must:\n",
    "- set the model to the `.eval()` mode\n",
    "- with torch.no_grad(), iterate on the iterator\n",
    "- calculate the prediction and the loss on the validation dataset\n",
    "- calculate epoch loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d12c09d5fa321a497e6ebeda2e4ecae6",
     "grade": false,
     "grade_id": "cell-49d6b9db12fefa95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(7 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "id": "ycVtF-7ppw1w",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b0b148f8239d4bea89bcd344c4eadc2a",
     "grade": true,
     "grade_id": "cell-63fd52b4acc96431",
     "locked": false,
     "points": 7,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            outputs = model(batch.text)\n",
    "\n",
    "            epoch_loss += criterion(outputs, batch.label)\n",
    "            epoch_acc += class_accuracy(outputs, batch.label)\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NnIUueopp2Mp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qCV2_q2qp9J7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50d0ce44f3c5f02519b98bb749f18e70",
     "grade": false,
     "grade_id": "cell-7319117f58f7a1d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will now train the module using the `FNNClassifier` model and the `train` and `evaluate` functions you have just implemented. Experiment with different setups as well. Try to change the number of epochs, the dimensions and the optimizer as well. Summarize your experience in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgNJd7sJp3yZ"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b437664e0769952f64809dbb973664fe",
     "grade": false,
     "grade_id": "cell-1ddce1c8e8f14563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Write your findings\n",
    "\n",
    "What are your findings while experimenting with different network settings, different loss functions, splits, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbb5e96aa8bb86158e9931eaf577f0bd",
     "grade": false,
     "grade_id": "cell-08b2211fef06112a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(6 points)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d6a7cf2f9b5eaf43e1aea8d837745e0",
     "grade": false,
     "grade_id": "cell-9f7e8dec2e5df289",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "WRITE YOUR ANSWER HERE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FNN model achieves a test set loss of 0.410 and 90.17% accuracy for the default parameters (15 epochs, Adam optimizer, NLLLoss). The validation set loss seemed to increase after 7 while the training set loss kept falling, which points to our model being overfitted.\n",
    "\n",
    "If we lower the number of epochs to 7, the test set loss and accuracy are 0.272 and 91.53%, respectively.\n",
    "\n",
    "Applying F.relu to the output of the linear layer did not seem to have much effect, while sigmoid worsened the results.\n",
    "\n",
    "Changing the optimizer to SGD with lr=1e-3 really worsened the results (loss above 1, accuracy around 45% through 15 epochs). With lr=1e-1 the results were better (0.7 and 73.81% test loss and accuracy through 15 epochs).\n",
    "RMSprop with its default parameters seemed promising, but the validation set loss kept increasing from the start. With lr=1e-3, a test set loss of 0.279 and accuracy of 90.53% are achieved at epoch 5, comparable to Adam.\n",
    "Adamax with default parameters performed similar to Adam in the end, but it took 15 epochs to achieve what Adam had in 7.\n",
    "\n",
    "If we use a 90/10 train/validation split instead of 70/30, with the other model parameters set to default, a low test set loss and accuracy (0.258, 91.91%) seem to be achieved earlier, around epoch 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5EdHOEIsr7KQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113ba4ee8487d571955a09160c009cb0",
     "grade": false,
     "grade_id": "cell-0554c2a95c00e0a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3. Change the Feed Forward network to LSTM. \n",
    "### (total per task 3: 20 points)\n",
    "\n",
    "To use an LSTM instead of a FNN for the classification task, you will need to change your `model`. \n",
    "\n",
    "* Add a `hidden_dim` parameter. The LSTM layer will produce a sentence vector from the word vectors. This parameter will represent the output of the LSTM layer.\n",
    "* Apply a linear layer to transform the feature vector from the LSTM output to class probabilities (It will still require Softmax)\n",
    "* Experiment with different layer sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "923249ce9a8e7ad56e435297311a0159",
     "grade": false,
     "grade_id": "cell-b2bcc113e1fbdadb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(15 points)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "Ra-XbbyCsscn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e602f17f5fa25cd7be87cf15a4a6cf94",
     "grade": true,
     "grade_id": "cell-e41d6ca828f61baf",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embeddings = self.embeddings(text)\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "        linear_out = self.linear(lstm_out[-1])\n",
    "        log_probs = F.log_softmax(linear_out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "LGs7Bmsrs2v1"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = LSTMClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAkoYkGVs4Np"
   },
   "source": [
    "Reuse the training and the evaluation functions from before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate optimizer, you can experiment with various optimizers: https://pytorch.org/docs/stable/optim.html\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Send the tensors to GPU if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 15\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "print(f'\\nTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4f5968d251cb38109c9265e47914226",
     "grade": false,
     "grade_id": "cell-187c52ace2166e2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Write your findings here\n",
    "\n",
    "How do the results of the LSTM and FNN compare? Timings, accuracy, etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a04bf9dcff012c06d8bb9358d4ee4c1",
     "grade": false,
     "grade_id": "cell-18dd672945066de3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_(5 pointw)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "37cca68721f3ed8754665839712880be",
     "grade": true,
     "grade_id": "cell-bf0e0287508e4f5f",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "LSTMClassifier took longer to train than FNN. An FNN epoch on Google Colab using GPUs took 7-8s, while an LSTM epoch took 12s.\n",
    "\n",
    "15 epochs also caused LSTM to overfit -- the lowest validation set loss seems to be achieved around epoch 5. Even then, however, this model actually performed worse than the simple FNNClassifier. The 5-epoch test set loss was 0.315 and the accuracy was 89.76%, compared to 0.272 and 91.53% for the default FNN model trained for 7 epochs.\n",
    "\n",
    "If we set LSTM's num_layers parameter to 2 (i.e. if we stack 2 LSTMs together), the best performance seems to happen at 3 epochs -- test set loss of 0.309 and 89.72% accuracy."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dl_homework2_adam.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
